![Problem](./images/lecture3/img1.JPG)

![Problem](./images/lecture3/img2.JPG)

![Problem](./images/lecture3/img3.JPG)

![Problem](./images/lecture3/img4.JPG)

![Problem](./images/lecture3/img5.JPG)

Probability of true class or p(c) is 1 and then we get the softmax loss.

![Problem](./images/lecture3/img6.JPG)

![Problem](./images/lecture3/img7.JPG)

![Problem](./images/lecture3/img8.JPG)

![Problem](./images/lecture3/img9.JPG)

![Problem](./images/lecture3/img10.JPG)

![Problem](./images/lecture3/img11.JPG)

![Problem](./images/lecture3/img12.JPG)

![Problem](./images/lecture3/img13.JPG)

![Problem](./images/lecture3/img14.JPG)

![Problem](./images/lecture3/img15.JPG)

![Problem](./images/lecture3/img16.JPG)

We can do classification for more complex task if we have deeper network.

![Problem](./images/lecture3/img17.JPG)

![Problem](./images/lecture3/img18.JPG)

Our general picture is well, we want to be able to do effective function approximation or curve fitting. We'd like to learn a space like this and we can only do that if we're sort of putting in some non-linearities which allow us to learn these kind of curvy decision, um, patterns. And so- so F is used effectively for doing accurate fu- function approximation or sort of pattern matching as you go along. 

![Problem](./images/lecture3/img19.JPG)

![Problem](./images/lecture3/img20.JPG)

![Problem](./images/lecture3/img21.JPG)

![Problem](./images/lecture3/img22.JPG)

![Problem](./images/lecture3/img23.JPG)

![Problem](./images/lecture3/img24.JPG)

Problem - we don't really know which of these words are meant to be classified.

![Problem](./images/lecture3/img25.JPG)

![Problem](./images/lecture3/img26.JPG)

![Problem](./images/lecture3/img27.JPG)

![Problem](./images/lecture3/img28.JPG)

![Problem](./images/lecture3/img29.JPG)

![Problem](./images/lecture3/img30.JPG)

![Problem](./images/lecture3/img31.JPG)

![Problem](./images/lecture3/img32.JPG)

![Problem](./images/lecture3/img33.JPG)

![Problem](./images/lecture3/img34.JPG)

So in the Jacobian, um, you're sort of taking these partial derivatives, um, with respect to each, um, output along the rows and with respect to each input down the columns. And so you're getting these m by n partial derivatives, considering every combination of an output and an input. 

![Problem](./images/lecture3/img35.JPG)

![Problem](./images/lecture3/img36.JPG)

![Problem](./images/lecture3/img37.JPG)

![Problem](./images/lecture3/img38.JPG)

![Problem](./images/lecture3/img39.JPG)

![Problem](./images/lecture3/img40.JPG)

![Problem](./images/lecture3/img41.JPG)

![Problem](./images/lecture3/img42.JPG)

![Problem](./images/lecture3/img43.JPG)

![Problem](./images/lecture3/img44.JPG)

![Problem](./images/lecture3/img45.JPG)

![Problem](./images/lecture3/img46.JPG)

![Problem](./images/lecture3/img47.JPG)

![Problem](./images/lecture3/img48.JPG)